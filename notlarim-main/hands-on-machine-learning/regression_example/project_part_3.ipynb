{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipynb.fs.full.project_part_1 import strat_train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s time to prepare the data for your Machine Learning algorithms. Instead of doing this manually, you should write functions for this purpose, for several good reasons:\n",
    "\n",
    "        - This will allow you to reproduce these transformations easily on any dataset (e.g., the next time you get a fresh dataset).\n",
    "        - You will gradually build a library of transformation functions that you can reuse in future projects.\n",
    "        - You can use these functions in your live system to transform the new data before feeding it to your algorithms.\n",
    "        - This will make it possible for you to easily try various transformations and see which combination of transformations works best.\n",
    "\n",
    "But first let’s revert to a clean training set (by copying `strat_train_set` once again). Let’s also separate the **predictors** and the **labels**, since we don’t necessarily want to apply the same transformations to the predictors and the target values (note that **`drop()`** creates a copy of the data and does not affect **`strat_train_set`**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels = strat_train_set['median_house_value'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. We saw earlier that the **`total_bedrooms`** attribute has some missing values, so let’s fix this. You have three options:\n",
    "\n",
    "        1. Get rid of the corresponding districts.\n",
    "        2. Get rid of the whole attribute.\n",
    "        3. Set the values to some value (zero, the mean, the median, etc.).\n",
    "\n",
    "You can accomplish these easily using DataFrame’s **`dropna()`**, **`drop()`**, and **`fillna()`** methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dropna(subset=['total_bedrooms'])   # Option 1\n",
    "housing.drop('total_bedrooms', axis=1)      # Option 2\n",
    "median = housing['total_bedrooms'].median() # Option 3\n",
    "housing['total_bedrooms'].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose option 3, you should compute the median value on the training set and use it to fill the missing values in the training set. Don’t forget to save the median value that you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data.\n",
    "\n",
    "Scikit-Learn provides a handy class to take care of missing values: **`SimpleImputer`**. Here is how to use it. First, you need to create a **`SimpleImputer`** instance, specifying that you want to replace each attribute’s missing values with the median of that attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the median can only be computed on numerical attributes, you need to create a copy of the data without the text attribute **`ocean_proximity`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.drop('ocean_proximity', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can fit the **`imputer`** instance to the training data using the **`fit()`** method:\n",
    "\n",
    "The **`imputer`** has simply computed the median of each attribute and stored the result in its **`statistics_`** instance variable. Only the **`total_bedrooms`** attribute had missing values, but we cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-118.51  ,   34.26  ,   29.    , 2119.5   ,  433.    , 1164.    ,\n",
       "        408.    ,    3.5409])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.fit(housing_num)\n",
    "\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-118.51  ,   34.26  ,   29.    , 2119.5   ,  433.    , 1164.    ,\n",
       "        408.    ,    3.5409])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use this “trained” **`imputer`** to transform the training set by replacing missing values with the learned medians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a plain NumPy array containing the transformed features. If you want to put it back into a pandas DataFrame, it’s simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index = housing_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only dealt with numerical attributes, but now let’s look at text attributes. In this dataset, there is just one: the **`ocean_proximity`** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6563</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12053</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13908</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11159</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15775</th>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ocean_proximity\n",
       "6563           INLAND\n",
       "12053          INLAND\n",
       "13908          INLAND\n",
       "11159       <1H OCEAN\n",
       "15775        NEAR BAY"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_cat = housing[['ocean_proximity']]\n",
    "housing_cat.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s not an arbitrary text: there are a limited number of possible values, each of which represents a category. So this attribute is a categorical attribute. Most Machine Learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s **`OrdinalEncoder`** class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [4.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the list of categories using the **`categories_`** instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad,” “average,” “good,” and “excellent”), but it is obviously not the case for the **`ocean_proximity`** column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1).\n",
    "\n",
    "**`One-Hot Encoding`**, only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called **dummy** attributes. Scikit-Learn provides a **`OneHotEncoder`** class to convert categorical values into one-hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16512 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is a SciPy **`sparse_matrix`**, instead of a NumPy array.\n",
    "\n",
    "This is very useful when you have categorical attributes with thousands of categories. After one-hot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements.\n",
    "\n",
    "You can use it mostly like a normal 2D array, but if you really want to convert it to a (dense) NumPy array, just call the **`toarray()`** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_cat_1hot.toarray() - You can do this if you want to change 'sparse_matrix' to the 'np.array'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, you can get the list of categories using the encoder’s\n",
    "\n",
    "**`categories_`** instance variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a categorical attribute has a large number of possible categories (e.g., country code, profession, species), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. If this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the `ocean_proximity` feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita). Alternatively, you could replace each category with a learnable, low-dimensional vector called an **embedding**. Each category’s representation would be learned during training. This is an example of **representation learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. You will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all you need to do is create a class and implement three methods:\n",
    "\n",
    " **`fit()`** (returning **self**), **`transform()`**, and **`fit_transform()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the transformer has one hyperparameter, `add_bedrooms_per_room`, set to `True` by default (it is often helpful to provide sensible defaults). This hyperparameter will allow you to easily find out whether adding this attribute helps the Machine Learning algorithms or not. More generally, you can add a hyperparameter to gate any data preparation step that you are not 100% sure about. The more you automate these data preparation steps, the more combinations you can automatically try out, making it much more likely that you will find a great combination (and saving you a lot of time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important transformations you need to apply to your data is **feature scaling**. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the target values is generally not required.\n",
    "\n",
    "There are two common ways to get all attributes to have the same scale: **min-max scaling (normalization)** and **standardization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (Min-Max Scaling)\n",
    "\n",
    "Min-max scaling (many people call this normalization) is the simplest: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called `MinMaxScaler` for this. It has a `feature_range` hyperparameter that lets you change the range if, for some reason, you don’t want 0–1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Standardization is different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers. For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0–15 down to 0–0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called `StandardScaler` for standardization.\n",
    "\n",
    "As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipelines\n",
    "\n",
    "Small pipeline for the numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. In version 0.20, Scikit-Learn introduced the `ColumnTransformer` for this purpose, and the good news is that it works great with pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "full_pipeline = ColumnTransformer([('num', num_pipeline, num_attribs), ('cat', OneHotEncoder(), cat_attribs)])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the `ColumnTransformer` class, next we get the list of numerical column names and the list of categorical column names, and then we construct a `ColumnTransformer`. The constructor requires a list of tuples, where each tuple contains a name,22 a transformer, and a list of names (or indices) of columns that the transformer should be applied to. In this example, we specify that the numerical columns should be transformed using the `num_pipeline` that we defined earlier, and the categorical columns should be transformed using a `OneHotEncoder`. Finally, we apply this `ColumnTransformer` to the housing data: it applies each transformer to the appropriate columns and concatenates the outputs along the second axis (the transformers must return the same number of rows).\n",
    "\n",
    "Note that the `OneHotEncoder` returns a `sparse matrix`, while the `num_pipeline` returns a `dense matrix`. When there is such a mix of sparse and dense matrices, the ColumnTransformer estimates the density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the density is lower than a given threshold (by default, sparse_threshold=0.3). In this example, it returns a dense matrix. And that’s it! We have a preprocessing pipeline that takes the full housing data and applies the appropriate transformations to each column.\n",
    "\n",
    "Instead of using a transformer, you can specify the string `\"drop\"` if you want the columns to be dropped, or you can specify `\"passthrough\"` if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to \"passthrough\") if you want these columns to be handled differently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
